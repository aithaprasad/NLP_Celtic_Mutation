{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BJsDHImZsd8x5VZXf6foogJyGVph3WUu",
      "authorship_tag": "ABX9TyPymF4pq5cZV9mmjHxsPedK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aithaprasad/NLP_Celtic_Mutation/blob/main/Celtic_Mutation_Scratch_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Reading the data from the respective files."
      ],
      "metadata": {
        "id": "GkoM9bV9Gn95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bDzJv0rhv2O9"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "with open('train.tsv', encoding=\"utf-8\") as file:\n",
        "  f = csv.reader(file, delimiter=\"\\t\")\n",
        "  all_data = []\n",
        "  for line in f:\n",
        "    all_data.append(line)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7sB4SrVxq00",
        "outputId": "3448f33d-c795-4fdd-8372-076da7a8b424"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5057059"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Function to just split the data. Re-used from the first competition"
      ],
      "metadata": {
        "id": "a3NGBEw_GzK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(data, percent = 0.1):\n",
        "    percent = int(percent * 10)\n",
        "\n",
        "    train = data[((len(data) // 10) * percent):]\n",
        "    test = data[:((len(data) // 10) * percent)]\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "SDZGj_dzyeFN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(all_data)"
      ],
      "metadata": {
        "id": "SN3FqCTZ0ACc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### As we have five tags. The below code keeps the count of different tags on each unique word. I guess the names for the variables speak for what they actually are."
      ],
      "metadata": {
        "id": "fj5YyPudG-2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tag_count = dict()\n",
        "N_count, S_count, U_count, T_count, H_count = 0, 0, 0, 0, 0 \n",
        "unique_words = []\n",
        "for word_tag in train:\n",
        "  word, tag = word_tag[0], word_tag[1]\n",
        "  \n",
        "  if word == \"<S>\": continue\n",
        "  \n",
        "  if tag == 'N': N_count += 1\n",
        "  elif tag == 'S': S_count += 1\n",
        "  elif tag == 'T': T_count += 1\n",
        "  elif tag == 'U': U_count += 1\n",
        "  else: H_count += 1\n",
        "\n",
        "  if word not in word_tag_count:\n",
        "    word_tag_count[word] = {tag : 1}\n",
        "    unique_words.append(word)\n",
        "  else:\n",
        "    if tag not in word_tag_count[word]: word_tag_count[word][tag] = 1\n",
        "    else: word_tag_count[word][tag] += 1"
      ],
      "metadata": {
        "id": "3hRALLA30PYZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_tag_count))\n",
        "print(len(unique_words))\n",
        "print(N_count)\n",
        "print(S_count)\n",
        "print(T_count)\n",
        "print(U_count)\n",
        "print(H_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLcdmOJ47iI0",
        "outputId": "75f41499-fd4d-4d59-e757-a1ee232fa4e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118690\n",
            "118690\n",
            "3725660\n",
            "443398\n",
            "16030\n",
            "149449\n",
            "36456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We are manipulating the above \"word_tag_count\" dictionary with the probabilities(each word's tag has the probability of that tag for that word). For example, the word \"tug\". it is a 'S' with 0.005895380673796454 probability."
      ],
      "metadata": {
        "id": "cycC1igOHX8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_count_per_tag = {'N' : N_count, 'S': S_count, 'T' : T_count, 'U': U_count, 'H' : H_count}\n",
        "for word in unique_words:\n",
        "  new_dict = {'N' : 0, 'S': 0, 'T' : 0, 'U': 0, 'H' : 0}\n",
        "  tags = ['N', 'S', 'U', 'T', 'H']\n",
        "  for tag in tags:\n",
        "    if tag in word_tag_count[word]:\n",
        "      new_dict[tag] = word_tag_count[word][tag] / total_count_per_tag[tag] \n",
        "  word_tag_count[word] = new_dict"
      ],
      "metadata": {
        "id": "kd31cO0l7rHx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_count_per_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBGqJvEkDi1S",
        "outputId": "5f440bc9-d6a5-4438-d31a-13eb3b92f1f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'N': 3725660, 'S': 443398, 'T': 16030, 'U': 149449, 'H': 36456}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tag_count['tug']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOo7r3tkDl0y",
        "outputId": "4a306bb9-1033-49ce-896b-6a1fcdee45fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'N': 2.1472705507212145e-06,\n",
              " 'S': 0.005895380673796454,\n",
              " 'T': 0,\n",
              " 'U': 0.0002944148170947949,\n",
              " 'H': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We are constructing the dictionary which stores the context for each sentence like how many times a 'N' has followed 'N' or basically, how many times what tag is followed by what tag. "
      ],
      "metadata": {
        "id": "YQDvasPbIpe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_context_tag_count = {'<S>' : {'N': 0, 'S': 0, 'T': 0, 'U': 0, 'H': 0}, 'N' : {'N': 0, 'S': 0, 'T': 0, 'U': 0, 'H': 0}, \n",
        "                              'S' : {'N': 0, 'S': 0, 'T': 0, 'U': 0, 'H': 0}, 'T' : {'N': 0, 'S': 0, 'T': 0, 'U': 0, 'H': 0},\n",
        "                              'U' : {'N': 0, 'S': 0, 'T': 0, 'U': 0, 'H': 0}, 'H' : {'N': 0, 'S': 0, 'T': 0, 'U': 0, 'H': 0}}\n",
        "\n",
        "for i in range(1, len(train)):\n",
        "  if train[i][0] == '<S>': continue\n",
        "  word, tag = train[i][0], train[i][1]\n",
        "  prev_word, prev_tag = train[i - 1][0], train[i - 1][1]\n",
        "  if prev_word == '<S>': prev_tag = '<S>'\n",
        "  sentence_context_tag_count[prev_tag][tag] += 1"
      ],
      "metadata": {
        "id": "SFgubqMuDqpS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_context_tag_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obKx9v40H55u",
        "outputId": "d79e5853-67c4-429b-e87e-4fa909ead584"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<S>': {'N': 162620, 'S': 17221, 'T': 14, 'U': 315, 'H': 190},\n",
              " 'N': {'N': 2953011, 'S': 395229, 'T': 15969, 'U': 148486, 'H': 35338},\n",
              " 'S': {'N': 417378, 'S': 22911, 'T': 28, 'U': 516, 'H': 692},\n",
              " 'T': {'N': 14970, 'S': 946, 'T': 3, 'U': 2, 'H': 41},\n",
              " 'U': {'N': 142934, 'S': 5702, 'T': 12, 'U': 100, 'H': 107},\n",
              " 'H': {'N': 34746, 'S': 1389, 'T': 4, 'U': 30, 'H': 88}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We utilize the count and convert them to probabilities with 1-smoothing."
      ],
      "metadata": {
        "id": "igwyg0J7JGX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tag in ['N', 'S', 'T', 'U', 'H', '<S>']:\n",
        "  total_tag_sum = sum(sentence_context_tag_count[tag].values())\n",
        "  for next_tag in ['N', 'S', 'T', 'U', 'H']:\n",
        "    sentence_context_tag_count[tag][next_tag] = (1 + sentence_context_tag_count[tag][next_tag]) / total_tag_sum"
      ],
      "metadata": {
        "id": "GpswQ9a4H8CH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_context_tag_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDwEthcMOHN7",
        "outputId": "3bda8854-6a02-42c1-ffd2-5b2643a2bb24"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<S>': {'N': 0.9016467065868263,\n",
              "  'S': 0.0954868041694389,\n",
              "  'T': 8.316699933466401e-05,\n",
              "  'U': 0.001752051452650255,\n",
              "  'H': 0.0010589931248613884},\n",
              " 'N': {'N': 0.8322955282546696,\n",
              "  'S': 0.111394116120115,\n",
              "  'T': 0.004501085531053404,\n",
              "  'U': 0.04185051266434106,\n",
              "  'H': 0.009960166661358561},\n",
              " 'S': {'N': 0.9453122699733877,\n",
              "  'S': 0.05189287129834098,\n",
              "  'T': 6.568144499178982e-05,\n",
              "  'U': 0.0011709416227846667,\n",
              "  'H': 0.0015695600475624256},\n",
              " 'T': {'N': 0.937915048239569,\n",
              "  'S': 0.05932840496178424,\n",
              "  'T': 0.0002505951635133442,\n",
              "  'U': 0.00018794637263500814,\n",
              "  'H': 0.002631249216890114},\n",
              " 'U': {'N': 0.9602297537872426,\n",
              "  'S': 0.038312451714755974,\n",
              "  'T': 8.733331094017668e-05,\n",
              "  'U': 0.0006785126465352188,\n",
              "  'H': 0.0007255382755030063},\n",
              " 'H': {'N': 0.9583528697906611,\n",
              "  'S': 0.03833742449733844,\n",
              "  'T': 0.00013790440466668506,\n",
              "  'U': 0.0008550073089334474,\n",
              "  'H': 0.002454698403066994}}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### This is basically the predict function which is Viterbi, where we utlize the above probabilities and assign the probability for the tag and store the max state(probability till that point) and at the end we return the sentence and the respective predicted tags."
      ],
      "metadata": {
        "id": "M4f1AsBMJQMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(sentence):\n",
        "  state = []\n",
        "  tags = ['N', 'S', 'U', 'T', 'H']\n",
        "  for key, word in enumerate(sentence):\n",
        "    p = []\n",
        "    for tag in tags:\n",
        "      emission_p = 0\n",
        "      if key == 0 or word == '<S>': transmission_prob = sentence_context_tag_count['<S>'][tag]\n",
        "      else: transmission_prob = sentence_context_tag_count[state[-1]][tag]\n",
        "      if word in word_tag_count.keys(): emission_p = word_tag_count[word][tag]\n",
        "  \n",
        "      state_prop = emission_p * transmission_prob\n",
        "      p.append(state_prop)\n",
        "    max_value=max(p)\n",
        "    value=tags[p.index(max_value)]\n",
        "    state.append(value)\n",
        "  return list(zip(sentence, state)), state"
      ],
      "metadata": {
        "id": "bh0aquW2OI_J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = []\n",
        "test_label = []\n",
        "for data in test:\n",
        "  test_data.append(data[0])\n",
        "  test_label.append(data[1])"
      ],
      "metadata": {
        "id": "Gzl8TZRnJ2rI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_pairs, pred_val = viterbi(test_data)"
      ],
      "metadata": {
        "id": "GJVutQKdQLcF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correct, total = 0, len(test_label)\n",
        "#for i in range(len(test_label)):\n",
        "#  if test_label[i] == pred_val[i]: correct += 1\n",
        "#print(correct / total)"
      ],
      "metadata": {
        "id": "exu9NjFkQ_az"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_from_scratch(sentence_tag):\n",
        "  \n",
        "  correct = 0\n",
        "  sentence, tags = [], []\n",
        "  \n",
        "  for word, tag in sentence_tag:\n",
        "    sentence.append(word)\n",
        "    tags.append(tag)\n",
        "  \n",
        "  pred_labels = viterbi(sentence)\n",
        "  for i in range(len(tags)):\n",
        "    if pred_labels[1][i] == tags[i]:\n",
        "      correct += 1\n",
        "  \n",
        "  return correct\n"
      ],
      "metadata": {
        "id": "hjtO3wy5T1e_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "  total = 0\n",
        "  correct_from_scratch = 0\n",
        "  #correct_anything_goes = 0\n",
        "  #testfile = open('test.tsv', 'r')\n",
        "  #testfile = open('drive/MyDrive/NLP_Assignments/celtic_train.tsv', 'r')\n",
        "  testfile = test#[0:505705]\n",
        "  sentence = []\n",
        "  for line in testfile:\n",
        "    total += 1\n",
        "    pieces = line #line.rstrip(\"\\n\").split(\"\\t\") \n",
        "    if pieces[0]=='<S>':\n",
        "      correct_from_scratch += predict_from_scratch(sentence)\n",
        "      #correct_anything_goes += predict_anything_goes(sentence)\n",
        "      sentence = []\n",
        "    else:\n",
        "      sentence.append(pieces)\n",
        "  correct_from_scratch += predict_from_scratch(sentence)\n",
        "  #correct_anything_goes += predict_anything_goes(sentence)\n",
        "  return correct_from_scratch/total#(, correct_anything_goes/total)"
      ],
      "metadata": {
        "id": "pF26Th8cRPRE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJsf9Z7rYLaJ",
        "outputId": "e7d14532-6971-4687-ac86-79967a7c2eb8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8695227454741401"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dl4VMpjhhUZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##References:\n",
        "\n",
        "https://www.mygreatlearning.com/blog/pos-tagging/ \n",
        "\n",
        "I used this for some algo understanding purposes, but did not take any code from it."
      ],
      "metadata": {
        "id": "qJ-GBE3Nj9ML"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcI80aWhkJF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}